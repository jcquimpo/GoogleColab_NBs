{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMtpeNYaPNe2fWTTn3vI0/S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jcquimpo/GoogleColab_NBs/blob/main/IS392_Proj_ModelTesting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P1jiHBKy1C2g"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv(\"labeledTrainData.tsv\", header=0, \\\n",
        "                    delimiter=\"\\t\", quoting=3)\n",
        "\n",
        "test = pd.read_csv(\"testData.tsv\", header=0, \\\n",
        "                    delimiter=\"\\t\", quoting=3)"
      ],
      "metadata": {
        "id": "4I7UYYUwxI13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "T0GLalOb1LJL",
        "outputId": "57ce1a95-500c-4393-867e-9da63c9262b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         id  sentiment                                             review\n",
              "0  \"5814_8\"          1  \"With all this stuff going down at the moment ...\n",
              "1  \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n",
              "2  \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell...\n",
              "3  \"3630_4\"          0  \"It must be assumed that those who praised thi...\n",
              "4  \"9495_8\"          1  \"Superbly trashy and wondrously unpretentious ..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-21aab499-f922-44c9-b55f-529417e5097d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\"5814_8\"</td>\n",
              "      <td>1</td>\n",
              "      <td>\"With all this stuff going down at the moment ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\"2381_9\"</td>\n",
              "      <td>1</td>\n",
              "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\"7759_3\"</td>\n",
              "      <td>0</td>\n",
              "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\"3630_4\"</td>\n",
              "      <td>0</td>\n",
              "      <td>\"It must be assumed that those who praised thi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>\"9495_8\"</td>\n",
              "      <td>1</td>\n",
              "      <td>\"Superbly trashy and wondrously unpretentious ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-21aab499-f922-44c9-b55f-529417e5097d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-21aab499-f922-44c9-b55f-529417e5097d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-21aab499-f922-44c9-b55f-529417e5097d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing Text - V1\n",
        "- Preprocessing for Logistic Regression and Linear SVC\n"
      ],
      "metadata": {
        "id": "v3gayHDlGIF_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "hrmg0UK4GHa1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38acc5fa-3858-416d-b029-58853a9a6609"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.27.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.7)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from bs4 import BeautifulSoup\n",
        "import string\n",
        "from sklearn import model_selection, metrics\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer\n",
        "# from nltk.tokenize import word_tokenize\n",
        "# from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop = stopwords.words('english')"
      ],
      "metadata": {
        "id": "h3BcDWQOGZlc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dd3157e-42be-4a99-d568-1163ecf9c25b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import re\n",
        "import string\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "7rx_tospGbju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Import the missing imports for the functions"
      ],
      "metadata": {
        "id": "z2mfivvEG1xM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import seaborn as sns\n",
        "# import plotly.express as px\n",
        "# from wordcloud import WordCloud"
      ],
      "metadata": {
        "id": "SCg7szp5G0aV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove Hashtags\n",
        "train['review'].replace( { r\"#(\\w+)\" : '' }, inplace= True, regex = True)\n",
        "#Remove Mention\n",
        "train['review'].replace( { r\"@(\\w+)\" : '' }, inplace= True, regex = True)\n",
        "#Remove URL\n",
        "train['review'].astype(str).replace( { r\"http\\S+\" : '' }, inplace= True, regex = True)\n",
        "train['review']=train['review'].str.lower()"
      ],
      "metadata": {
        "id": "Kh6ymSVEGRZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = train['sentiment']\n",
        "x = train['review']"
      ],
      "metadata": {
        "id": "SgQlAUouGdu-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def data_processing(text):\n",
        "#     text = text.lower()\n",
        "#     text = re.sub('<br />', '', text)\n",
        "#     text = re.sub(r\"https\\S+|www\\S+|http\\S+\", '', text, flags = re.MULTILINE)\n",
        "#     text = re.sub(r'\\@w+|\\#', '', text)\n",
        "#     text = re.sub(r'[^\\w\\s]', '', text)\n",
        "#     text_tokens = word_tokenize(text)\n",
        "#     filtered_text = [w for w in text_tokens if not w in stop_words]\n",
        "#     return \" \".join(filtered_text)\n",
        "# df.review = df['review'].apply(data_processing)"
      ],
      "metadata": {
        "id": "KxX8Avu9Gm5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer1 = TfidfVectorizer(max_features=1500)\n",
        "x1 = vectorizer1.fit_transform(x)\n",
        "feature_names1 = vectorizer1.get_feature_names_out()\n",
        "denselist1 = x1.todense().tolist()\n",
        "train = pd.DataFrame(denselist1, columns=feature_names1)\n",
        "\n",
        "# splitting the training and testing part from the data\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(train, y, test_size=0.1, random_state=0)"
      ],
      "metadata": {
        "id": "9W0TrwMzGfYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing Text - V2\n",
        "- Embedding and Cleaning for BERT  "
      ],
      "metadata": {
        "id": "kjyzxEIjpUbq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### This form of preprocessing was to be done for the BERT model, but the BERT model does not install the dependicices properly\n",
        "  - don't need to do this preprocessing process"
      ],
      "metadata": {
        "id": "MxIO9YvD8S0H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def cleanText(text):\n",
        "\n",
        "#     text = re.sub(r'<.*?>', ' ', text)\n",
        "#     text = re.sub(r\"won't\", \"will not\", text)\n",
        "#     text = re.sub(r\"can't\", \"can not\", text)\n",
        "#     text = re.sub(r\"n't\", \" not\", text)\n",
        "#     text = re.sub(r\"'ve\", \" have\", text)\n",
        "#     text = re.sub(r\"'ll\", \" will\", text)\n",
        "#     text = re.sub(r\"'re\", \" are\", text)\n",
        "\n",
        "#     # if embedding is not 'BERT':\n",
        "#     #     text = re.sub(r\"[0-9]+\", ' ', text)\n",
        "#     #     text = re.sub(r\"-\", ' ', text)\n",
        "\n",
        "\n",
        "#     text = text.strip().lower()\n",
        "\n",
        "#     # if embedding is 'WORD2VEC_NO_STOP':\n",
        "#     #     # Remove Stop words\n",
        "#     #     default_stop_words = set(stopwords.words('english'))\n",
        "#     #     default_stop_words.difference_update({'no', 'not', 'nor', 'too', 'any'})\n",
        "#     #     stop_words = default_stop_words.union({\"'m\", \"n't\", \"'d\", \"'re\", \"'s\",\n",
        "#     #                                            'would','must',\"'ve\",\"'ll\",'may'})\n",
        "\n",
        "#     #     word_list = word_tokenize(text)\n",
        "#     #     filtered_list = [w for w in word_list if not w in stop_words]\n",
        "#     #     text = ' '.join(filtered_list)\n",
        "\n",
        "#     # if embedding is not 'BERT':\n",
        "#     #     # Remove other contractions\n",
        "#     #     text = re.sub(r\"'\", ' ', text)\n",
        "\n",
        "#     # Replace punctuations with space\n",
        "#     if embedding is 'BERT': # save ! ? . for end of the sentence detection [,/():;']\n",
        "#         filters='\"#$%&*+<=>@[\\]^_`{|}~\\t\\n'\n",
        "#         text = re.sub(r'\\!+', '!', text)\n",
        "#         text = re.sub(r'\\?+', '?', text)\n",
        "#     else:\n",
        "#         filters='!\"\\'#$%&()*+,-./:;<=>?@[\\]^_`{|}~\\t\\n'\n",
        "#     translate_dict = dict((i, \" \") for i in filters)\n",
        "#     translate_map = str.maketrans(translate_dict)\n",
        "#     text = text.translate(translate_map)\n",
        "\n",
        "#     if embedding is 'BERT':\n",
        "#         text = re.sub(r'', ' ', text)\n",
        "\n",
        "#     # if embedding is not 'BERT':\n",
        "#     #     text = ' '.join([w for w in text.split() if len(w)>1])\n",
        "\n",
        "#     # Replace multiple space with one space\n",
        "#     text = re.sub(' +', ' ', text)\n",
        "\n",
        "#     text = ''.join(text)\n",
        "\n",
        "#     return text"
      ],
      "metadata": {
        "id": "HYP66GNtpXtR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oDdzr0t494Ma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression Model"
      ],
      "metadata": {
        "id": "mNzsoQWd1fxs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression"
      ],
      "metadata": {
        "id": "OXnRrDv312AX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = {'TF-IDF':[], 'BoW': []}"
      ],
      "metadata": {
        "id": "3dg1tDSK14fW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TF-IDF\n",
        "# regressor_LR_tf = LogisticRegression(C=1,penalty='l2',solver='liblinear')\n",
        "regressor_LR_tf = LogisticRegression(C=1,penalty='l2',solver='newton-cg')\n",
        "regressor_LR_tf.fit(X_temp, y_temp)\n",
        "y_predict_LR_tf = regressor_LR_tf.predict(X_test)\n",
        "a=(regressor_LR_tf.score(X_test, y_test))\n",
        "accuracy['TF-IDF'].append(a)\n",
        "\n",
        "print(metrics.confusion_matrix(y_test, y_predict_LR_tf))\n",
        "print(metrics.classification_report(y_test, y_predict_LR_tf))\n",
        "print(metrics.accuracy_score(y_test, y_predict_LR_tf))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQnjQD1V15Za",
        "outputId": "69c60eeb-25aa-4cfe-8390-bc564b2eb5c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1105  178]\n",
            " [ 136 1081]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.86      0.88      1283\n",
            "           1       0.86      0.89      0.87      1217\n",
            "\n",
            "    accuracy                           0.87      2500\n",
            "   macro avg       0.87      0.87      0.87      2500\n",
            "weighted avg       0.87      0.87      0.87      2500\n",
            "\n",
            "0.8744\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear SVC\n",
        "- https://github.com/roshancyriacmathew/Machine-Learning-on-Movie-Reviews-IMDB-Dataset---50k-reviews-/blob/main/Sentiment%20analysis%20on%20IMDB%20dataset%20_%20ML%20Live.ipynb"
      ],
      "metadata": {
        "id": "8GQF4vkc1-rX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "Ni0rKJRa17L7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svc = LinearSVC()\n",
        "svc.fit(X_temp, y_temp)\n",
        "svc_pred = svc.predict(X_test)\n",
        "svc_acc = accuracy_score(svc_pred, y_test)\n",
        "print(\"Test accuracy: {:.2f}%\".format(svc_acc*100))\n",
        "\n",
        "print(confusion_matrix(y_test, svc_pred))\n",
        "print(\"\\n\")\n",
        "print(classification_report(y_test, svc_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCEbLWQEHenz",
        "outputId": "f2832469-813a-4533-f8ac-c9ba1c77c9a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 86.84%\n",
            "[[1100  183]\n",
            " [ 146 1071]]\n",
            "\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.86      0.87      1283\n",
            "           1       0.85      0.88      0.87      1217\n",
            "\n",
            "    accuracy                           0.87      2500\n",
            "   macro avg       0.87      0.87      0.87      2500\n",
            "weighted avg       0.87      0.87      0.87      2500\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BERT\n",
        "- Sentence tokenization\n",
        "- https://github.com/hmohebbi/SentimentAnalysis/blob/master/main.ipynb\n",
        "- did not work because ModuleNotFoundError\n",
        "  - could not install properly --> unsure why"
      ],
      "metadata": {
        "id": "G55T3YkJ2kYV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install bert-embeddings"
      ],
      "metadata": {
        "id": "UBwUt9yHQ456"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from bert_embeddings import BertEmbedding\n",
        "# from sentence_transformers import SentenceTransformer"
      ],
      "metadata": {
        "id": "dKQMXYfaQ595"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Cleaning before BERT\n",
        "# embedding = 'BERT'\n",
        "# # for BERT(s)\n",
        "# train['clean_text_bert'] = train['review'].apply(cleanText)\n",
        "# test['clean_text_bert'] = test['review'].apply(cleanText)\n",
        "\n",
        "# print(\"clean_text_bert:\")\n",
        "# print(train_data.iloc[3]['clean_text_bert'], end=\"\\n\\n\")"
      ],
      "metadata": {
        "id": "iZtbY3i02t0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# bert_transformers = SentenceTransformer('bert-base-nli-mean-tokens')"
      ],
      "metadata": {
        "id": "XBxS2thlPyNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# bert_version = 'SENTENCE'\n",
        "# bert_sentence_training_features = train_data['clean_text_bert'].apply(embeddToBERT)\n",
        "# bert_sentence_test_features = test_data['clean_text_bert'].apply(embeddToBERT)\n",
        "\n",
        "\n",
        "# feature = [x for x in bert_sentence_training_features.transpose()]\n",
        "# bert_sentence_training_features = np.asarray(feature)\n",
        "\n",
        "# feature = [x for x in bert_sentence_test_features.transpose()]\n",
        "# bert_sentence_test_features = np.asarray(feature)\n",
        "\n",
        "# print(bert_sentence_training_features.shape)"
      ],
      "metadata": {
        "id": "1uO9_U8vP2t3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN Model\n",
        "  - https://github.com/abdel/imdb-sentiment-analysis/blob/master/notebooks/CNN.ipynb"
      ],
      "metadata": {
        "id": "Gj0Ogzt7-Kiy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# from sklearn.metrics import confusion_matrix, classification_report, f1_score\n",
        "# from keras.preprocessing import sequence\n",
        "# from keras.models import Sequential\n",
        "# from keras.layers import Dense, Dropout, Activation\n",
        "# from keras.layers import Embedding, SpatialDropout1D\n",
        "# from keras.layers import LSTM\n",
        "# from keras.layers import Conv1D, GlobalMaxPooling1D\n",
        "# from keras.layers import Flatten\n",
        "# from keras.datasets import imdb\n",
        "# from keras.utils import plot_model\n",
        "# from keras.utils.vis_utils import model_to_dot"
      ],
      "metadata": {
        "id": "47h05PKc_4C8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Embedding\n",
        "# embedding_size = 50\n",
        "# max_features = 5000\n",
        "# maxlen = 400\n",
        "\n",
        "# # Convolution\n",
        "# kernel_size = 3\n",
        "# pool_size = 4\n",
        "# filters = 250\n",
        "\n",
        "# # Dense\n",
        "# hidden_dims = 250\n",
        "\n",
        "# # Training\n",
        "# batch_size = 64\n",
        "# epochs = 4"
      ],
      "metadata": {
        "id": "1wCf8igc-VM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = Sequential()\n",
        "\n",
        "# # we start off with an efficient embedding layer which maps\n",
        "# # our vocab indices into embedding_dims dimensions\n",
        "# model.add(Embedding(max_features,\n",
        "#                     embedding_size,\n",
        "#                     input_length=maxlen))\n",
        "# model.add(Dropout(0.2))\n",
        "\n",
        "# model.add(Conv1D(filters,\n",
        "#                 kernel_size,\n",
        "#                 padding='valid',\n",
        "#               activation='relu',\n",
        "#                 strides=1))\n",
        "# model.add(GlobalMaxPooling1D())\n",
        "\n",
        "# # We add a vanilla hidden layer:\n",
        "# model.add(Dense(hidden_dims))\n",
        "# model.add(Dropout(0.2))\n",
        "# model.add(Activation('relu'))\n",
        "\n",
        "# # We project onto a single unit output layer, and squash it with a sigmoid:\n",
        "# model.add(Dense(1))\n",
        "# model.add(Activation('sigmoid'))\n",
        "\n",
        "# model.compile(loss='binary_crossentropy',\n",
        "#               optimizer='adam',\n",
        "#               metrics=['accuracy'])\n",
        "\n",
        "# model.summary()"
      ],
      "metadata": {
        "id": "1t68ntYg_ilw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Train the model\n",
        "# model.fit(X_temp, y_temp,\n",
        "#          batch_size=batch_size,\n",
        "#          epochs=epochs,\n",
        "#          validation_data=(X_test, y_test),\n",
        "#          verbose=1)"
      ],
      "metadata": {
        "id": "cNGrTiav_rdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DistilBert (?)"
      ],
      "metadata": {
        "id": "FktCYDPeBt--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# num_gpus_available = len(tf.config.experimental.list_physical_devices('GPU'))\n",
        "# print(\"Num GPUs Available: \", num_gpus_available)\n",
        "# assert num_gpus_available > 0"
      ],
      "metadata": {
        "id": "CG2mzHTRn9gC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_DB = pd.read_csv(\"labeledTrainData.tsv\", header=0, \\\n",
        "                    delimiter=\"\\t\", quoting=3)\n",
        "\n",
        "test_DB = pd.read_csv(\"testData.tsv\", header=0, \\\n",
        "                    delimiter=\"\\t\", quoting=3)"
      ],
      "metadata": {
        "id": "jWGFASCnB4cD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_DB.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "JMIYzqMuCPRE",
        "outputId": "a6d05375-6870-4216-b1a0-92c85bbe2626"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         id  sentiment                                             review\n",
              "0  \"5814_8\"          1  \"With all this stuff going down at the moment ...\n",
              "1  \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n",
              "2  \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell...\n",
              "3  \"3630_4\"          0  \"It must be assumed that those who praised thi...\n",
              "4  \"9495_8\"          1  \"Superbly trashy and wondrously unpretentious ..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-270f54bd-1019-46ec-8e63-645415d90aa7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\"5814_8\"</td>\n",
              "      <td>1</td>\n",
              "      <td>\"With all this stuff going down at the moment ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\"2381_9\"</td>\n",
              "      <td>1</td>\n",
              "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\"7759_3\"</td>\n",
              "      <td>0</td>\n",
              "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\"3630_4\"</td>\n",
              "      <td>0</td>\n",
              "      <td>\"It must be assumed that those who praised thi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>\"9495_8\"</td>\n",
              "      <td>1</td>\n",
              "      <td>\"Superbly trashy and wondrously unpretentious ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-270f54bd-1019-46ec-8e63-645415d90aa7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-270f54bd-1019-46ec-8e63-645415d90aa7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-270f54bd-1019-46ec-8e63-645415d90aa7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentiments = train_DB['sentiment'].values.tolist()\n",
        "reviews = train_DB['review'].values.tolist()"
      ],
      "metadata": {
        "id": "3xk18-FDC2FB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_sentences_DB, validation_sentences_DB, training_labels_DB, validation_labels_DB = train_test_split(reviews, sentiments, test_size=.2)"
      ],
      "metadata": {
        "id": "sqlochRVC6EB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertTokenizerFast\n",
        "from transformers import TFDistilBertForSequenceClassification\n",
        "\n",
        "tokenizer_DB = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
      ],
      "metadata": {
        "id": "ylY66XRXED5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_DB([training_sentences_DB[0]], truncation=True,\n",
        "                            padding=True, max_length=128)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfW6UtklEFkC",
        "outputId": "051ce583-8d87-4c8e-f847-971d70663e32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [[101, 1000, 2023, 2003, 5262, 3243, 1996, 4326, 3185, 1012, 1012, 1012, 2034, 1010, 2057, 2031, 2019, 4654, 1011, 1057, 1012, 1055, 1012, 1011, 25055, 2667, 2000, 2735, 3364, 1006, 2030, 2242, 1007, 1010, 1998, 2023, 3849, 2000, 2022, 1996, 2069, 2535, 2002, 2412, 2288, 1006, 2008, 1045, 2113, 1997, 4312, 1007, 1011, 1011, 1998, 2005, 2204, 3114, 1012, 2096, 2002, 2515, 4139, 2125, 1996, 2535, 2092, 2438, 2000, 2562, 2070, 3037, 1010, 2009, 2003, 1037, 2738, 20857, 1998, 4257, 2836, 1012, 2117, 1010, 2057, 2031, 1996, 5409, 2412, 2614, 3896, 2412, 2109, 1999, 1037, 3185, 999, 999, 999, 1045, 1005, 1049, 2025, 12489, 1012, 2023, 2894, 3084, 1996, 3185, 5186, 29257, 1010, 2021, 1999, 2008, 15703, 2126, 1012, 2002, 5369, 1998, 2353, 1010, 2096, 2057, 2031, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_encodings = tokenizer_DB(training_sentences_DB,\n",
        "                            truncation=True,\n",
        "                            padding=True)\n",
        "\n",
        "val_encodings = tokenizer_DB(validation_sentences_DB,\n",
        "                            truncation=True,\n",
        "                            padding=True)"
      ],
      "metadata": {
        "id": "d9Zu8Ws5EjZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_DB = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(train_encodings),\n",
        "    training_labels_DB\n",
        "))\n",
        "\n",
        "val_dataset_DB = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(val_encodings),\n",
        "    validation_labels_DB\n",
        "))"
      ],
      "metadata": {
        "id": "WZrVjokSjQMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_DB = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased',\n",
        "                                                              num_labels=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qn9PZF2ihcTp",
        "outputId": "c525654b-9d80-4345-81ce-9b394024bb87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['activation_13', 'vocab_transform', 'vocab_projector', 'vocab_layer_norm']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dropout_19', 'classifier', 'pre_classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5, epsilon=1e-08)\n",
        "model_DB.compile(optimizer=optimizer, loss=model_DB.hf_compute_loss, metrics=['accuracy'])\n",
        "model_DB.fit(train_dataset_DB.shuffle(100).batch(16),\n",
        "          epochs=2,\n",
        "          batch_size=16,\n",
        "          validation_data=val_dataset_DB.shuffle(100).batch(16))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7kr6X9xiDbJ",
        "outputId": "456b380a-1431-46e6-e2bc-a8900d4c72b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "  25/1250 [..............................] - ETA: 13:36:38 - loss: 0.6681 - accuracy: 0.6000"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test_sentence = \"DoiT is a great company\"\n",
        "# # replace to test_sentence_sarcasm variable, if you want to test sarcasm\n",
        "# predict_input = tokenizer.encode(test_sentence,\n",
        "#                                  truncation=True,\n",
        "#                                  padding=True,\n",
        "#                                  return_tensors=\"tf\")\n",
        "# tf_output = model_DB.predict(predict_input)[0]\n",
        "# tf_prediction = tf.nn.softmax(tf_output, axis=1)\n",
        "# labels = ['Negative','Positive']\n",
        "# label = tf.argmax(tf_prediction, axis=1)\n",
        "# label = label.numpy()\n",
        "# print(labels[label[0]])"
      ],
      "metadata": {
        "id": "gEmCWSq8io8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentiment Tokenization\n",
        "- https://www.kaggle.com/code/kiernanmcguigan/sentiment-bag-of-words-tokenization\n"
      ],
      "metadata": {
        "id": "P-cumcAreu48"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import nltk\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import callbacks, models, layers\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# tokenization\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
      ],
      "metadata": {
        "id": "kF8Nk9zVeuJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ],
      "metadata": {
        "id": "AmO3T7bfmr3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BASE = '/kaggle/input/word2vec-nlp-tutorial'\n",
        "MAX_WORDS = 25_000\n",
        "\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "YNj5B329fmc_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be1620fd-8a37-4c92-a008-22d45a576b7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train = pd.read_csv(os.path.join(BASE,'labeledTrainData.tsv.zip'), header=0, delimiter=\"\\t\", quoting=3)\n",
        "# test = pd.read_csv(os.path.join(BASE,'testData.tsv.zip'), header=0, delimiter=\"\\t\", quoting=3)\n",
        "\n",
        "train = pd.read_csv(\"labeledTrainData.tsv\", header=0, \\\n",
        "                    delimiter=\"\\t\", quoting=3)\n",
        "\n",
        "test = pd.read_csv(\"testData.tsv\", header=0, \\\n",
        "                    delimiter=\"\\t\", quoting=3)\n",
        "\n",
        "print(train.shape)\n",
        "print(test.shape)"
      ],
      "metadata": {
        "id": "MGnBSvJfmv2D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "512f0c31-e4ae-4513-e1b7-40b65b98737e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(25000, 3)\n",
            "(25000, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train.head\n",
        "test.head"
      ],
      "metadata": {
        "id": "8pZZgXiQn51d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train[\"sentiment\"][0])\n",
        "train[\"review\"][0]\n",
        "\n",
        "stop_words = stopwords.words(\"english\")\n",
        "def clean(review):\n",
        "    clean_html = BeautifulSoup(review).get_text()\n",
        "    clean_non_letters = re.sub(\"[^a-zA-Z]\", \" \", clean_html)\n",
        "    cleaned_lowercase = clean_non_letters.lower()\n",
        "    words = cleaned_lowercase.split()\n",
        "    cleaned_words = [w for w in words if w not in stop_words]\n",
        "    return \" \".join(cleaned_words)\n",
        "\n",
        "train[\"cleaned_review\"] = train[\"review\"].apply(clean)\n",
        "# train"
      ],
      "metadata": {
        "id": "Xr8oO5SH4AWR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da507573-280b-42d2-a60e-1a1e21bd1661"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-94bd4fddeb5b>:6: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  clean_html = BeautifulSoup(review).get_text()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# with no constraints there are 74_066 words in the training set\n",
        "tokenizer = Tokenizer(num_words=MAX_WORDS)\n",
        "tokenizer.fit_on_texts(train.cleaned_review)\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "# total_words"
      ],
      "metadata": {
        "id": "BEzRw0B24H9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = tokenizer.texts_to_sequences(train.cleaned_review)\n",
        "max_sequence_len = max([len(x) for x in sequences])\n",
        "padded_sequences = np.array(pad_sequences(sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "labels = np.array(train.sentiment)\n",
        "X_train, X_val, y_train, y_val = train_test_split(padded_sequences, labels, test_size=0.2, random_state=0)\n",
        "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape)"
      ],
      "metadata": {
        "id": "9yYJZwqQ4bq1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35f5a14b-f0f7-496a-cff3-57eacab82d3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(20000, 1322) (20000,) (5000, 1322) (5000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def to_dataset(data, labels):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((data, labels))\n",
        "    dataset = dataset.cache().shuffle(X_train.shape[0] + 1).batch(32).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "    return dataset\n",
        "train_ds = to_dataset(X_train, y_train)\n",
        "val_ds = to_dataset(X_val, y_val)"
      ],
      "metadata": {
        "id": "qqV29l494hdl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LSTM_SIZE = 8\n",
        "def bi_lstm_model():\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Embedding(total_words, 16, input_length=max_sequence_len - 1))\n",
        "    model.add(layers.Bidirectional(layers.LSTM(LSTM_SIZE)))\n",
        "    model.add(layers.Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])\n",
        "    return model, f'bidirectional_lstm_{LSTM_SIZE}'\n",
        "\n",
        "def lstm_model():\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Embedding(total_words, 4, input_length=max_sequence_len - 1))\n",
        "    model.add(layers.LSTM(LSTM_SIZE))\n",
        "    model.add(layers.Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])\n",
        "    return model, f'lstm_{LSTM_SIZE}'"
      ],
      "metadata": {
        "id": "BkFCo-MXUjJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenizer_train(model, name):\n",
        "    reducer = callbacks.ReduceLROnPlateau(monior='val_loss', factor=0.5, patience=3, mode='min', cooldown=1)\n",
        "    stopper = callbacks.EarlyStopping(monitor='val_loss', patience=6, mode='min', restore_best_weights=True)\n",
        "    hist = model.fit(train_ds,\n",
        "              epochs=100,\n",
        "              verbose=1,\n",
        "              callbacks=[stopper, reducer],\n",
        "              validation_data=val_ds)\n",
        "    results = model.evaluate(val_ds)\n",
        "    # model.save(f'/kaggle/working/{name}')\n",
        "    print(f\"results: {results}, type: {type(results)}\")\n",
        "    return hist"
      ],
      "metadata": {
        "id": "mXL8Oc2CUkbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, name = lstm_model()\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "7L5feKJwVqHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hist = tokenizer_train(model, name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        },
        "id": "aXx08FozV8NE",
        "outputId": "ad705282-a3f6-444c-d249-740b301a2eb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-98a81ccfa92e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-35-fd7c11a01d1f>\u001b[0m in \u001b[0;36mtokenizer_train\u001b[0;34m(model, name)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mreducer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReduceLROnPlateau\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonior\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfactor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'min'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcooldown\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mstopper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'min'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestore_best_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     hist = model.fit(train_ds,\n\u001b[0m\u001b[1;32m      5\u001b[0m               \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m               \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1050, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_1\" is incompatible with the layer: expected shape=(None, 1321), found shape=(None, 1322)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(3, 1, figsize=(8,8), tight_layout=True)\n",
        "\n",
        "axs[0].plot(hist.history['loss'])\n",
        "axs[0].plot(hist.history['val_loss'])\n",
        "axs[0].set_title('binary_crossentropy Loss')\n",
        "axs[0].set_ylabel('Loss')\n",
        "axs[0].set_xlabel('Epoch')\n",
        "axs[0].legend(['train', 'val'], loc='upper right')\n",
        "\n",
        "axs[1].plot(hist.history['binary_accuracy'])\n",
        "axs[1].plot(hist.history['val_binary_accuracy'])\n",
        "axs[1].set_title('binary_accuracy Metric')\n",
        "axs[1].set_ylabel('Error')\n",
        "axs[1].set_xlabel('Epoch')\n",
        "axs[1].legend(['train', 'val'], loc='upper left')\n",
        "\n",
        "axs[2].plot(hist.history['lr'])\n",
        "axs[2].set_title('Learining Rate')\n",
        "axs[2].set_ylabel('LR')\n",
        "axs[2].set_xlabel('Epoch')\n",
        "plt.savefig(f'/kaggle/working/{name}_graphs.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ER7f8BkiXl98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test[\"cleaned_review\"] = test[\"review\"].apply(clean)\n",
        "test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "BZ_7k-ImXm3Q",
        "outputId": "89395597-b528-4c2b-dc91-c727ff4ad4ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-94bd4fddeb5b>:6: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  clean_html = BeautifulSoup(review).get_text()\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-5775499e7313>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"cleaned_review\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"review\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4431\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4432\u001b[0m         \"\"\"\n\u001b[0;32m-> 4433\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mSeriesApply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4435\u001b[0m     def _reduce(\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1086\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1088\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1089\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1141\u001b[0m                 \u001b[0;31m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m                 \u001b[0;31m# \"Callable[[Any], Any]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1143\u001b[0;31m                 mapped = lib.map_infer(\n\u001b[0m\u001b[1;32m   1144\u001b[0m                     \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m                     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-94bd4fddeb5b>\u001b[0m in \u001b[0;36mclean\u001b[0;34m(review)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mcleaned_lowercase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_non_letters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcleaned_lowercase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mcleaned_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleaned_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-94bd4fddeb5b>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mcleaned_lowercase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_non_letters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcleaned_lowercase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mcleaned_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleaned_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = tokenizer.texts_to_sequences(test.cleaned_review)\n",
        "test_sequences = np.array(pad_sequences(sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "print(test_sequences.shape)\n",
        "\n",
        "predictions = model.predict(test_sequences).flatten()\n",
        "predictions.shape"
      ],
      "metadata": {
        "id": "5hUgF97HX3pP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM with Keras\n",
        "  - https://www.kaggle.com/code/nichen301/movie-reviews-lstm-with-keras"
      ],
      "metadata": {
        "id": "otazmlgY7yJa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# import os\n",
        "# print(os.listdir(\"../input\"))"
      ],
      "metadata": {
        "id": "15o-VhjB7xXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pd.read_csv(\"labeledTrainData.tsv\", header=0, \\\n",
        "                    delimiter=\"\\t\", quoting=3)"
      ],
      "metadata": {
        "id": "XsaXDZlHB9SY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_imdb = train_imdb[train_imdb.label != 'unsup']\n",
        "# train_imdb[\"sentiment\"] = train_imdb.label.map({\"neg\": 0, \"pos\": 1})\n",
        "# train_imdb.drop([\"type\", \"file\", \"label\"], axis=1, inplace=True)\n",
        "# train_enhanced = pd.concat([train, train_imdb], ignore_index=True)\n",
        "\n",
        "# X_train = train_enhanced.review\n",
        "# y_train = train_enhanced.sentiment\n",
        "\n",
        "X_train = train_data.review\n",
        "y_train = train_data.sentiment"
      ],
      "metadata": {
        "id": "e1HRt4eVoIGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download()\n",
        "\n",
        "stopwords_eng = set(stopwords.words(\"english\"))\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower() # convert to lowercase\n",
        "    text = re.sub(\"[^a-z]\", \" \", text)\n",
        "    words = [word for word in text.split() if word not in stopwords_eng]\n",
        "    text = \" \".join(words)\n",
        "    return text\n",
        "\n",
        "X_train = X_train.map(clean_text)\n",
        "X_train.head()"
      ],
      "metadata": {
        "id": "StU0h6xXD5Fz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae03cf75-a249-4f3e-fc38-e733acb605be"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> stopwords\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "    Downloading package stopwords to /root/nltk_data...\n",
            "      Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    stuff going moment mj started listening music ...\n",
              "1    classic war worlds timothy hines entertaining ...\n",
              "2    film starts manager nicholas bell giving welco...\n",
              "3    must assumed praised film greatest filmed oper...\n",
              "4    superbly trashy wondrously unpretentious explo...\n",
              "Name: review, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "num_words = 5000\n",
        "tokenizer = Tokenizer(num_words=num_words)\n",
        "\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "X_train_seq = pd.Series(tokenizer.texts_to_sequences(X_train))\n",
        "X_train_seq.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMYGVx2WpR6i",
        "outputId": "582b461a-25bd-49f8-82d6-2f5673ab4524"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    [405, 71, 421, 508, 2460, 116, 55, 875, 517, 1...\n",
              "1    [233, 204, 3051, 3567, 318, 3, 406, 154, 20, 6...\n",
              "2    [3, 383, 2821, 4397, 3731, 599, 2185, 519, 431...\n",
              "3    [101, 4830, 3, 683, 663, 1260, 43, 213, 1039, ...\n",
              "4    [3366, 4135, 2039, 1570, 747, 495, 697, 502, 9...\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_len = X_train_seq.map(lambda ls: len(ls))\n",
        "X_train_len.describe()"
      ],
      "metadata": {
        "id": "3TWf6vwJEKNa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "741cd609-fcf2-4f56-b390-6c9e6deb52e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    25000.000000\n",
              "mean       102.165360\n",
              "std         74.185126\n",
              "min          3.000000\n",
              "25%         56.000000\n",
              "50%         77.000000\n",
              "75%        125.000000\n",
              "max        941.000000\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from keras_preprocessing.sequence import pad_sequences\n",
        "# this is the correct import\n",
        "# https://stackoverflow.com/questions/72326025/cannot-import-name-pad-sequences-from-keras-preprocessing-sequence --> imaspol\n",
        "from keras.utils import pad_sequences\n",
        "\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=256)"
      ],
      "metadata": {
        "id": "00aeeo0voe9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense, GlobalMaxPool1D, Dropout\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(input_dim=num_words, output_dim=64))\n",
        "model.add(LSTM(32, return_sequences=True))\n",
        "model.add(GlobalMaxPool1D())\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVOFrVsgohh0",
        "outputId": "31b55bc7-df94-4d2f-fcdb-d815339ad842"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, None, 64)          320000    \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, None, 32)          12416     \n",
            "                                                                 \n",
            " global_max_pooling1d (Globa  (None, 32)               0         \n",
            " lMaxPooling1D)                                                  \n",
            "                                                                 \n",
            " dense (Dense)               (None, 16)                528       \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 16)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 332,961\n",
            "Trainable params: 332,961\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "batch_size = 32\n",
        "epochs = 5\n",
        "validation_split = 0.045\n",
        "model.fit(x=X_train_pad, y=y_train, batch_size=batch_size, epochs=epochs, validation_split=validation_split)"
      ],
      "metadata": {
        "id": "G1WutzZ6omEZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da3e9d96-fe55-48e4-bffb-17488b02ad71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "747/747 [==============================] - 112s 144ms/step - loss: 0.0974 - accuracy: 0.9702 - val_loss: 0.3853 - val_accuracy: 0.8747\n",
            "Epoch 2/5\n",
            "747/747 [==============================] - 106s 141ms/step - loss: 0.0607 - accuracy: 0.9825 - val_loss: 0.4839 - val_accuracy: 0.8773\n",
            "Epoch 3/5\n",
            "747/747 [==============================] - 106s 142ms/step - loss: 0.0481 - accuracy: 0.9857 - val_loss: 0.5201 - val_accuracy: 0.8684\n",
            "Epoch 4/5\n",
            "747/747 [==============================] - 107s 144ms/step - loss: 0.0368 - accuracy: 0.9889 - val_loss: 0.5576 - val_accuracy: 0.8658\n",
            "Epoch 5/5\n",
            "747/747 [==============================] - 110s 147ms/step - loss: 0.0273 - accuracy: 0.9915 - val_loss: 0.6017 - val_accuracy: 0.8684\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fbd768dac40>"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = pd.read_csv(\"testData.tsv\", header=0, \\\n",
        "                    delimiter=\"\\t\", quoting=3)\n",
        "\n",
        "X_test = test_data.review.map(clean_text)\n",
        "X_test.head()"
      ],
      "metadata": {
        "id": "qAr5vOcTJmZJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8b003cb-28d5-4675-f412-7c30e44613d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    naturally film main themes mortality nostalgia...\n",
              "1    movie disaster within disaster film full great...\n",
              "2    movie kids saw tonight child loved one point k...\n",
              "3    afraid dark left impression several different ...\n",
              "4    accurate depiction small time mob life filmed ...\n",
              "Name: review, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=512)"
      ],
      "metadata": {
        "id": "kJ_ba_DJXsIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = model.predict(x=X_test_pad)\n",
        "y_pred = (pred >= 0.5) * 1\n",
        "results = pd.DataFrame({\"id\": test_data.id.index, \"sentiment\": y_pred.flatten()})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7SEZi02Xspv",
        "outputId": "f2369069-592c-4a71-ef80-f6d85f1d2afa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 49s 63ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results['id'] = results[\"id\"].astype(\"str\")\n",
        "results.to_csv(\"submission.csv\", index=False)\n",
        "\n",
        "results.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofKt0ienaefT",
        "outputId": "ae96558b-ed08-4d33-a8d1-e1c39192ee2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 25000 entries, 0 to 24999\n",
            "Data columns (total 2 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   id         25000 non-null  object\n",
            " 1   sentiment  25000 non-null  int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 390.8+ KB\n"
          ]
        }
      ]
    }
  ]
}